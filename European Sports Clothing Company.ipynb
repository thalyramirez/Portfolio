'''
# Analysis of The Company: an European sports clothes company
Globalization has brought facilities for businesses to take advantages over evolving markets with favorable conditions to grow in terms of profitability, brand recognition, and competitiveness. By expanding their markets, the business is healthy diversifying its operations against the risky reliance over similar market niches that may be already saturated.

After recognizing that value, The Company decided to expand internationally an is looking for a country where they can succeed in the retail sport clothing industry. This analysis has detected two main business opportunities of The Company that help to identify the ideal country to expand.

## Problem 1

### Problem statement

The Company has a particular diversification in India's market as their sells in this country depend mainly on traditional clothes such as kurtas more than sport clothes. However, its stock levels are risky low to supply its demand in the future as India represents a huge market of more than 20 million units sold only in one of those categories. 

Following its willingness to expand to new markets, the business should look for countries with cultural affinity towards these types of clothes. In this way, economy of scale can be boosted with higher production levels for both sport and Indian clothes that not only engage each type of consumer within the other products, but also increase the business management of the new market meanwhile maintaining its core competencies.



### Understanding

The last report available shows that The Company portfolio in a market as India contains 21 categories in which more than 70% of the stock available is not related to sport clothes but to traditional clothes such as kurtas, sarees, and shararas. Likewise,  those clothes represents more than 50% of its sells, meanwhile sport clothes is barely 8% in the Indian market. Despite these facts, the business only manage 0.6% of the products on stock which may generate lost sales and increased costs while trying to supply the demand. 

Looking for a market with similar features to India is also important as doing business in this country seems to be challenging for clothing retailers in general, as this market has reported low profits in segments such as Men's clothing where the average profit per order was -29.65 USD by 2018. 

By internationalizing, the company can enter a suitable market size with Indian influences where the increased demand of traditional clothes reinforce the economy of scale also for sporting clothes. Also, this can be an opportunity to overcome the low profitability in its original market.

## Problem 2

### Problem statement

Even though the business has a diversified clients portfolio, there has been a volatile monthly demand since 2021 where few months have high peeks that drop drastically and does not replicate until the next year. With this happening, it may be imperative to increase the customers outside Europe to make its solidify its demand and reduce risky across the year seasons.


### Understanding
With more than 140 unique clients, The Company has a healthy diversification of its primary income as they purchased more than 30 million units between 2021 and 2022 and there is no customer representing more than 15% of the sales, from which we can affirm that the business has solid main income as it is supported by a wide variety of customer across the countries it currently operates.
However, this demand is instable across the year as During 2022 and 2021, the sells registered shows a high demand
in the months of September and February. These situations, together with the high inventory stock analyzed previously and a significant drop in sales for 2022 of more than six million units, indicates that the company should leverage a solid demand that increases the peeks across the year and reduces the falls levels. Expanding operations implicates higher and more stable demands if the country's development variables suggest a favorable market size.

# Results
It is highly recommended to expand to Trinidad and Tobago, in Latin America & Caribbean. This country not only have optimal and superior demographic and socio-economic factors to find a good market, but also shares cultural similarities with the current markets where The Company is operating.


## Problem 1
Trinidad and Tobago is a country with a wide ethnic diversity: external sources register that 35% (Brereton, s.f.) of its population is East Indian. This represents a huge opportunity for The Company to approach that market segment with an existing offer of its traditional clothes meanwhile it is introducing itself as a reference for sport clothings for the general consumers. This facility not only allows the good reception of the brand but also helps to reduce the stock risk on India's market where it is already positioned.

Regarding this market purchase capabilities, Trinidad and Tobago is a wealthy country with one of the highest GNI available of $17,010 USD and high Human Development Index (0.79/1), which means that its population enjoy of "a long and healthy life, being knowledgeable and having a decent standard of living" (UNDP, 2021). This indicates that its population -including the Indian consumers- enjoy of wealthy capabilities and purchasing power to compete with a differentiation strategy that supports the low profits in India for both sport and traditional categories.

As the Company has plenty experience over the European and some Asian markets where it is positioned as a strong reference in the sport clothing industries. However, they lacks of knowledge to properly incurse into new markets makes necesary to start finding a country where The Company can introduce its expertise with less risk of being rejected by the market.




## Problem 2
Tobago and Trinidad socio-economic conditions are suitable to find a wealthy market size that increases the diversification of customers for The Company and improve the demand distribution across the year with lower gaps between peaks and valleys.

Let's start by highlighting that 70% of Trinidad and Tobago's population is between 15 and 64 years, with a middle age of 36, which represents a large and growing consumer base. Secondly, this country has one of the highest Human Development -explained in Problem 1- together with a high Human Asset Index (94/100) that measures the welfare in terms of education, health, poverty, and mortality. 

The analysis also showed that those two main indicators are positively correlated to the GNI, life expentancy, and Quality of life of a country, and negatively correlated with the unemployment rates even within countries where those variables are medium or low. As there are data limitations to know more about the prospective market, it is valid to infer from those correlations that this country has wealthy prospect market of at least 1 million consumers with a high purchasing power.


As a consequence of accessing this new market, The Company can keep increasing the diversification of customer in new regions that also increases the demand across the year to reduce the risk of economic downturns and changes in consumer behavior in Europe countries or India's markets. However, as there is no information about the behavior of the clothing or retail industry in this country, it is not possible to determine how the new overall demand will behave, for example, with the same high-demand months or new ones.

# Assumptions made


## Problem 1
Due to the high amount of data available of India's market (amazon e-commerce sells) and the categories of Indian traditional clothes, we assume that the company has its manufacturing center there. Furthermore, The Company took advantage of it to approach the Indian market and a way it could adapt to it was by introducing new production lines of traditional clothes.


## Problem 2
When analyzing the company performance and current status, the datasets didn't have the same timeline or it was unknown. Therefore, it was assumed that in the last 6 years the market conditons remained almost the same.

Likewise, to merge the countries information into a centralized table, it was assumed that the information available correspond to the same period of time, most likely 2020.



# Limitations


## Problem 1
The data available does not represent the sells in currency but in units, and the highest accuracy over the business performance in current markets is more likely to be lead towards India's market than Europe's.

Here it is important to highlight that the data available is not enough to determine the costs of shipping the production on these product categories overseas.

## Problem 2
The data available to measure the countries wealth was not available for all countries. Therefore, the final list of options compiled the most accurate information taking into account a relevance order for some variables which availability reduced traditional markets such as US. Those null values weren't replaced because tying to predict its values wouldn't be the best option as we are trying with real world indicators that includes lots of other economics variables that, in the other hand, have been calculated but are not included in the datasets available so far.


# Data


## Dataset 1
'int_sale' dataset contains information about The Company's sells (by orders) and customers, together with the date when the order was placed and the products included.
This dataset was used for problem 1 to identify the dependency of the company over some categories and make the assumption over India's market presence. Also, it was used for problem 2 to scan the demand behavior of the business and the customer's diversification.
One of the main constrains of the dataset is that there are mixed columns such as Months, DATE, and CUSTOMER that were important for the analysis. Later, there were duplicated customers that weren't removed by a groupby fuction. Therefore, it was necessary to do a split of the customers' names and then apply a new groupby. This implies that there may be more customers than the final result, but the risk was accepted.
Assuming the column RATE of the original dataset refers to the quantity of products ordered.

## Dataset 2
'sale_report' contains The Company's orders with specific information about the products on stock (designs, categories, size, and color).
This dataset was used to identify the categories of clothes The Company offers and the stock available for each one.
The main constrain on it are the huge amount of null rows, but they were dropped without affecting the analysis quality.

## Dataset 3
'amazon_sales' contains information about The Company's sells on Amazon in India. It was used for problem 1 to scan the consumer behavior -especially their favorability to certain categories-.
This dataset presented several constrains: first, it wasn't possible to identify a column with the sales in currency and, even given, it wouldn't be possible to convert the currency with an exchange rate. Therefore, it was assumed that the Amount column represented the amount of items sold. Furthermore, there were some transactions cancelled that couldn't constitute real sells so it was necessary to clean and transform the column.

## Dataset 4
'data_supply' contains sells transactions of the retail industry around the world. It includes, besides other things, the origin country and the profits per order, which were of interest for this analysis. That is why this dataset didn't present major constrains but it was necessary to make the assumption that its data represents the overall market behavior and not only for US and PR as they are the Customer countries.
The dataset was used for problem 1 to determine the average profitability of India's market in clothing categories.

## Datasets (7 sets)
'indicators', 'wb_population', 'life_exp', 'dep_countries','age', 'unemp', and 'quality_life' are six independent datasets from the World Bank that contains demographic variables of countries around the world, useful to analyze the welfare of a nation. Their variables were used for the problem 2 approach as criteria to find the best country where to internationalize. Also, to gather information about describing the advantages of Trinidad and Tobago for the problem 1.

Some of the constrains found were related to data types that had to be changed and a considerable amount of null values across the datasets as not all the information was available for some countries. However, dropping rows would decrease even more the amount of countries, and trying to predict its values wouldn't be the best option as we are trying with real world indicators that includes lots of other economics variables that, in the other hand, have been calculated but are not included in the datasets available so far.


An assumption about the time line of all the information was made to locate it between 2021 and 2022 because most of the datasets didn't indicate the year of the information collected. In other cases, the information was from more than 5 years ago so we assume that the conditions keep being similar.

# Problem Solving

'''

#Importing packages
import pandas as pd
import numpy as np
import sqlite3
import json
import seaborn as sns
import datetime as dt
import matplotlib.pyplot as plt
import zipfile as zipfile

#Defining fuction to check on duplicates
def unique_values(x,y):
    #Listing the unique values in a specific column (y) of a dataframe (x)
    s = pd.Series(x[y].unique())
    #Counting how many unique values there is to compare with the table info
    length = len(s)
    return length

## Problem 2

#Loading the dataset
int_sale_zip = zipfile.ZipFile('./A1/International sale Report.csv.zip')
int_sale = pd.read_csv(int_sale_zip.open('International sale Report.csv'), encoding = 'iso-8859-1')

In order to understand the behavior of The Company's demand, the dataset was cleaned and re-organized to show the monthly sales during 2021 and 2022.

#To analyze the data, cleaned columns: RATE, Date, Customer, Style, and SIZE are needed
#Creating a new dataframe to not change the original one in case a restart is needed.
int_sale_report=int_sale

#DATE, CUSTOMER, and Size columns have mixed values from other columns. Data types needs to be changed (to strings) to separate the values
int_sale_report['DATE']=int_sale_report['DATE'].astype(str)
int_sale_report['CUSTOMER']=int_sale_report['CUSTOMER'].astype(str)
int_sale_report['Size']=int_sale_report['Size'].astype(str)

#Creating new columns where the values are reasigned correctly. Splited by separators
int_sale_report['Customer']= int_sale_report.apply(lambda x:
                                                   #Assigning the  value in the DATE column if the separator is not included because it is a customer
                                                   x['DATE']
                                                   if '-' not in x['DATE'] 
                                                   #If there is a separator that means that it is a mixed value and the correct one is in another column
                                                   else x['CUSTOMER'], 
                                                   axis=1)
int_sale_report['Date']= int_sale_report.apply(lambda x: 
                                               #Assigning the value in the other Customer column if the separator is  included because it means there is a date
                                               x['CUSTOMER']
                                               if '-' in x['CUSTOMER']
                                               #If there isn't a separator that means that it is a correct value
                                               else x['Months'],
                                               axis=1)
int_sale_report['SIZE']= int_sale_report.apply(lambda x: 
                                               #Assigning a Null value to those cells with numerical characters
                                               None
                                               if '.' in x['Size']
                                               #Leaving the same values if there is no signal of numerical characters
                                               else x['Size'], 
                                               axis=1)

#Renaming the column RATE to make it more clear
int_sale_report=int_sale_report.rename(columns={'RATE':'Qty'})
#Making sure the column Qty is numeric to make aggregation fuctions later
int_sale_report['Qty'] = pd.to_numeric(int_sale_report['Qty'],
                                       #If the value can't be converted, then replace with a Null
                                       errors='coerce')

#Redefining the dataframe with the columns needed
int_sale_report=int_sale_report[['Customer',
                                 'Date',
                                 'SIZE',
                                 'Qty',
                                 'Style']]

#Spliting the month and year from the column Date.
int_sale_report[['Month','Year']]= int_sale_report['Date'].str.split('-',
                                                                     #Creating two new columns called Month and Year
                                                                     expand=True)

#Defining the datatype as string for the Month to define new column later
int_sale_report['Month']=int_sale_report['Month'].astype(str)

#Creating new column with numeric values for the column Month, to stablish an order when plotting
int_sale_report['month']=int_sale_report.apply(lambda x:
                                               1 if 'Jan' in x['Month']
                                               else (2 if 'Feb' in x['Month'] 
                                                     else (3 if 'Mar' in x['Month'] 
                                                           else (4 if 'Apr' in x['Month'] 
                                                                 else (5 if 'May' in x['Month']
                                                                       else (6 if 'Jun' in x['Month'] 
                                                                             else (7 if 'Jul' in x['Month'] 
                                                                                   else (8 if 'Aug' in x['Month'] 
                                                                                         else (9 if 'Sep' in x['Month'] 
                                                                                               else (10 if 'Oct' in x['Month'] 
                                                                                                     else (11 if 'Nov' in x['Month'] 
                                                                                                           else (12 if 'Dec' in x['Month'] 
                                                                                                                 #If the value doesn't match a valid string, then replace with a Null
                                                                                                                 else None))))))))))),
                                               axis=1)
#Droping old column to avoid saturating the table
int_sale_report=int_sale_report.drop(['Month'], 
                                     axis='columns')

#Changing new Year and month columns data types into numeric to plot it
int_sale_report['Year'] = pd.to_numeric(int_sale_report['Year'],
                                        #If the value can't be converted, then replace with a Null
                                        errors='coerce')
int_sale_report['month'] = pd.to_numeric(int_sale_report['month'],
                                         #If the value can't be converted, then replace with a Null
                                         errors='coerce')

#Filtering the table into a new dataframe with rows that shows the year of the sells and customer with more than 1 Qty
isr_filtered = int_sale_report.where(int_sale_report['Qty'] >0)
#Assigning full year
isr_filtered['year']=isr_filtered.apply(lambda x: 2021
                                        if x['Year']==21 
                                        else 
                                        2022, axis=1)

#Final table with needed columns
isr = pd.DataFrame(isr_filtered.groupby(['year','month'])['Qty'].sum())

#Graph with monthly sells (Qty) registered
plott = isr.plot(title='Units sold 2021-2022')

The previous graph shows the sells seasonalities where there are high demand only in the months of September and February followed by drastic falls.

Next, the annual sales may be compared to keep diagnosing the business.

#Grouping sells by year
isr_year_sells = pd.DataFrame(isr_filtered.groupby(['year'])['Qty'].sum()).reset_index()
#Pivoting the column year
isr_year_sells1=isr_year_sells.pivot_table(columns='year', 
                                           values='Qty')
#Calculating the sells' change
sells_change=(isr_year_sells1[2022]-isr_year_sells1[2021])
#Calculating the percentual change
sc_percentage=(isr_year_sells1[2022]-isr_year_sells1[2021])/isr_year_sells1[2021]*100
performance={'Sales change':sells_change,'Percentage':sc_percentage}
performance

The Company's sales dropped almost 35%.
However, this may be because there is no data for most months of 2022.

Now, it was analyzed the customer's dataset to scan the diversification of the business and check on the previous demand behavior dependency to some customers. 

#Preparing Customer column to be cleaned from duplicated by Converting values to lowercase
int_sale_report['Customer']=int_sale_report['Customer'].str.lower()

#New table with Customer and quantity purchased
isr_customer=pd.DataFrame(int_sale_report.groupby('Customer')
                          ['Qty'].sum().sort_values(ascending=False)).reset_index()

#Checking unique values and possible duplicates
unique_values(isr_customer,'Customer')

#Spliting the Customer column as there are duplicates that the groupby didn't sum (161 non-unique customers)
customers_name=isr_customer['Customer'].str.split(' ',
                                                  #Creting a new column for each part splited
                                                  expand=True)

#Keeping only the first column to identify unique customers
customers_name=customers_name.drop([1,2,3,4,5],
                                   axis='columns')

#Concatenating the isr_customer table with splited one now that the order and number of rows are the same
isr_split_cx=pd.concat([isr_customer,
                        customers_name],
                       axis=1)

#Removing the old Customer column
isr_split_cx=isr_split_cx.drop(['Customer'], 
                               axis='columns')

#Renaming the new Customer column
isr_split_cx=isr_split_cx.rename(columns={0:'Customer'})

#New table grouped where customers' puchases are summed 
isr_customer_no_duplicates=pd.DataFrame(isr_split_cx.groupby('Customer')
                                        ['Qty'].sum().sort_values(ascending=False)).reset_index()

#Total units sold
isr_total_sells=round(isr_customer_no_duplicates['Qty'].sum(),2)

#Adding new column with percentages: individual purchases over total sells
isr_customer_no_duplicates['Percentage']= isr_customer_no_duplicates['Qty'].div(isr_total_sells).mul(100)

#Excluding possible outliers or unrelevant data were customers participation is less than 0%
isr_customer_no_duplicates = isr_customer_no_duplicates.where(isr_customer_no_duplicates['Percentage'] >0)

#Droping null values to get a accurate number of unique customers (142 unique customers)
isr_customer_no_duplicates = isr_customer_no_duplicates.dropna()

#Confirming 100% percentage
total_percetange=isr_customer_no_duplicates['Percentage'].sum()

#Counting number of unique values
n_customers=isr_customer_no_duplicates['Customer'].count()
#Getting the maximum portion of the sales that a customer have
max_customer= round(isr_customer_no_duplicates['Percentage'].max(),2)
#Pulling the calculations together
customers_info= ({'Total sells':[isr_total_sells],'Number of customers':[n_customers],'Max. weight':[max_customer]})
customers_info
'''
The Company has at least 142 high-volume customers representing over 30 million units sold. Hopefully, the business demand does not depend heavely in any customer as the maximum portion of those sells attributed to a customer is 13%, which means that the company has a wealthy diversification regardless the unstable demand.

The main analysis was done to determine which country has the best conditions to internationalize the business. Six databases were used for this purpose by merging them into a centralized table to run a correlation matrix that helps to identify the key variables that favor the best country.
'''
#Loading the dataset
wb_population = pd.read_csv('./A1/population-by-country-2020/population_by_country_2020.csv', encoding = 'iso-8859-1')

#Getting the relevant columns
population=wb_population[['Country (or dependency)',
                          'Population (2020)',
                          'Yearly Change',
                          'Migrants (net)',
                          'Med. Age','Urban Pop %']]

#Cleaning columns that are suppose to have numbers but includes characters
#Creating new columns for the splited values
population[['Population Change','%']]=population['Yearly Change'].str.split(' ',
                                                                            expand=True)
population[['Urban Change','%']]=population['Urban Pop %'].str.split(' ',
                                                                     expand=True)

#Changing the data types to numeric
population['Population Change']=pd.to_numeric(population['Population Change'],
                                              errors='coerce')

population['Urban Change']=pd.to_numeric(population['Urban Change'],
                                         errors='coerce')

#Grouping the cleanned columns
countries_pop=population[['Country (or dependency)',
                          'Population (2020)',
                          'Population Change',
                          'Urban Change',
                          'Migrants (net)',
                          'Med. Age']]
#Changing column name for easier merge
countries_pop=countries_pop.rename(columns={'Country (or dependency)':
                                            'ShortName'})

#Ordering the values
countries_pop.sort_values(by=['ShortName'],
                          ascending=True)

#Loading the dataset
life_exp = pd.read_csv('./A1/life_expectancy.csv', encoding = 'iso-8859-1')

#Melting columns into labels to use aggregations
le_country=life_exp.melt(id_vars=['Country Code'],
                         value_vars=['2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016'], 
                         value_name='life expentancy')

#Grouping average life expentancies by country as the last year registred was from 7 years ago
le_country_avg=pd.DataFrame(le_country.groupby('Country Code')
                            ['life expentancy'].mean().reset_index())

#Renaming column to do merge later
le_country_avg=le_country_avg.rename(columns={'Country Code':'CountryCode'})

#Rounding life expentancy
le_country_avg['life expentancy']=round(le_country_avg['life expentancy'],2)

le_country_avg

#Loading data
dep_countries = pd.read_csv('./A1/list_of_developed_or_developing_countries.csv', encoding = 'iso-8859-1')

#Getting the columns needed 
development=dep_countries.drop('Population 2020', axis=1)

#Renaming columns to merge later
development=development.rename(columns={'Country Name':'ShortName'})

#Cheking on duplicates
unique_values(development,'ShortName')
    #262 rows from which 147 are unique. There are duplicates

#Droping duplicated rows when they have a Null value
development=development.dropna()
#Checking on duplicates
unique_values(development,'ShortName')
    #108 rows, no duplicates

#Rename columns to make changes
unique_development=development.rename(columns={'GNI per capita':'GNI'})

#Spliting values to convert column into numeric values
unique_development[['GNI per capita','CurrencyGNI']]= unique_development['GNI'].str.split(' ',expand=True)

#Droping old columns
unique_development=unique_development.drop('GNI', axis=1)

unique_development

#Loading data
age = pd.read_csv('./A1/countries-dataset-2020/Coutries age structure.csv')

#Checking on duplicates: 191 uniques
unique_values(age,'Country')
    #191 rows, no duplicates

#Selecting columns
population_15_64y=age[['Country','Age 15 to 64 Years']]

#Renaming columns to merge later
p1564y=population_15_64y.rename(columns={'Country':'ShortName'})

p1564y

#Loading data
unemp = pd.read_csv('./A1/oecd-unemployment-rate-2020/2020 unemployment.csv')

#Checking on duplicates: 40 uniques
unique_values(unemp,'LOCATION')
    #40 rows, no duplicates

#Creating dataframe with average unemployment rates by country
unemployment=pd.DataFrame(unemp.groupby('LOCATION')['Value'].mean().reset_index())

#Renaming columns to merge later
country_unemp=unemployment.rename(columns={'LOCATION':'CountryCode','Value':'Unemployment'})
#Rounding column
country_unemp['Unemployment']=round(country_unemp['Unemployment'],2)

country_unemp.head(10)

#Loading data
quality_life = pd.read_csv('./A1/countries-dataset-2020/Quality of life index by countries 2020.csv')

#Checking on duplicates: 80 uniques
unique_values(quality_life,'Country')
    #80 rows, no duplicates

#Selecting useful columns
ql_indicators=quality_life[['Country',
                            'Quality of Life Index', 
                            'Purchasing Power Index', 
                            'Safety Index',
                            'Cost of Living Index',
                            'Health Care Index']]

#Renaming colums to merge later
qli=ql_indicators.rename(columns={'Country':'ShortName'})

qli

#Loading data
indicators = pd.read_csv('./A1/world-development-indicators/Country.csv')

#MERGING TABLES

#Selecting useful columns form reference table. This table will be used to match all the other tables
ind_table=indicators[['CountryCode','ShortName','CurrencyUnit','Region','IncomeGroup']]


leca_universal=pd.merge(ind_table,le_country_avg, on='CountryCode')
#Renaming column to differenciate from the ones that will be created automatically later, so it will not be removed when dropping them.
leca_universal=leca_universal.rename(columns={'Region':'CountryRegion'})


cp_universal=pd.merge(ind_table,countries_pop, on='ShortName')


development_universal=pd.merge(ind_table,unique_development, on='ShortName')



qli_universal=pd.merge(ind_table,qli, on='ShortName')


p1564y_universal=pd.merge(ind_table,p1564y, on='ShortName')


cu_universal=pd.merge(ind_table,country_unemp, on='CountryCode')


#Merging all the tables to match the countries with its indicator
merging_tables=pd.merge(leca_universal,
                        cp_universal, 
                        on='ShortName', 
                        how='outer').merge(development_universal,
                                           on='ShortName', 
                                           how='outer').merge(qli_universal,
                                                              on='ShortName', 
                                                              how='outer').merge(p1564y_universal,
                                                                                 on='ShortName',
                                                                                 how='outer').merge(cu_universal,
                                                                                                    on='ShortName', how='outer')

#Droping the colums duplicated from the merge by position
countries_indicators=merging_tables.drop(merging_tables.columns[[6,7,8,9,15,16,17,18,24,25,26,27,33,34,35,36,38,39,40,41]],
                                         axis=1)



#Separating numerical from non-numerical characters
countries_indicators['GNI per capita'] = countries_indicators['GNI per capita'].str.replace(',', '')
countries_indicators['Age 15 to 64 Years'] = countries_indicators['Age 15 to 64 Years'].str.replace('%', '')

# Converting columns into numeric values
countries_indicators['GNI per capita'] = pd.to_numeric(countries_indicators['GNI per capita'],
                                                       errors='coerce')
countries_indicators['Med. Age'] = pd.to_numeric(countries_indicators['Med. Age'],
                                                 errors='coerce')
countries_indicators['Age 15 to 64 Years'] = pd.to_numeric(countries_indicators['Age 15 to 64 Years'],
                                                           errors='coerce')

#Checking duplicates. 242 unique values
unique_values(countries_indicators,'ShortName')
    #242 rows, no duplicates.

#Making sure the datatype of the Region
countries_indicators['CountryRegion']=countries_indicators['CountryRegion'].astype(str)

#Creating a new column to identify countries from Europe
countries_indicators['Europe']=countries_indicators.apply(lambda x: 1 if 'Europe' in x['CountryRegion'] else 0, axis=1)

#Checking on data types
countries_indicators.info()

#Selecting columns for correlation matrix
ci_columns1=countries_indicators.drop(['Population (2020)'],
                                      axis=1)

#Saving the correlation matrix
corr1 = ci_columns1.corr()
#Fixing dimensions of the matrix
plt.figure(figsize=(12, 10))

#As a general rule of thumb, a minimum of 30 observations is often suggested for a reliable estimate of a correlation coefficient. 

#Executing the correlation map with relevant correlations
sns.heatmap(corr1[(corr1 >= 0.5) | (corr1 <= -0.5)],
            cmap='viridis',
            vmax=1.0,
            vmin=-1.0,
            linewidths=0.1,
            annot=True,
            annot_kws={"size": 10},
            square=True);

'''
5 factors indicates which range of countries are the most suitable to go global with with the data available. These factors, in order of importance, are: Human Development Index, GNI per capita, Population Change, Age 15 to 64 Years, and Human Asset Index.

Others factors are also considered but, because of significant differences in the data available, the correlations within the previous 5 will be the most accurate guide to describe further behaviors of the country later.

Because there are no significant differences within Human Development Indexes, the variable will be used to filter the countries with high human development (0.7-0.79).
'''

#First filter to define a country: excluding European countries
countries_filter1=countries_indicators[(countries_indicators['Europe']<1)]

#Second filter to define a country: High Human Development Index (higher than 0.7)
countries_filter1=countries_filter1[(countries_filter1['Human Development Index']>0.7)]

#Only 2 countries registeres unemployment rate. Column be droped. All countries are in a Developed or Developing status, will be droped 
countries_filter2=countries_filter1.drop(['Migrants (net)',
                                          'CurrencyGNI',
                                          'Europe',
                                          'Unemployment',
                                          'Development Status'], 
                                         axis=1).reset_index()

#Third filter: High GNI per capita, life expentancy, productive population, and population change. The matrix suggests >0.7 correlations
countries_filter3=countries_filter2.sort_values(by=['GNI per capita',
                                                    'life expentancy',
                                                    'Age 15 to 64 Years',
                                                    'Population Change'],
                                                ascending=[False,False,False,True]).head(10)

countries_filter3
'''
Trinidad and Tobago is the best country to introduce the business.
'''
## Problem 1
Looking at what the business produces and sells is a valuable start to know its performance in the current markets.

#Loading the dataset
sale_report = pd.read_csv('./A1/Sale Report.csv')

#Droping rows with full NaN values. Previous rows: 9271, new: 9188. Saving into new dataframe to conserve the original
b_categories=sale_report.dropna()

#Redefining dataframe with the columns of interest
b_categories= b_categories[['Category',
                            'Stock']]

#Saving into a dataframe the results of grouping the stock by category
b_designs=pd.DataFrame(b_categories.groupby('Category')
                       ['Stock'].sum().reset_index())

#Calculating the total stock
total_stock=b_designs['Stock'].sum()

#Creating a new column with the percentages of each category over the total stock
b_designs['Stock %']=round(b_designs['Stock']/total_stock*100,2)

#Sorting the table by stock percentage
b_designs.sort_values(by=['Stock %'], 
                      ascending=False)
'''
The table shows that at least 10 out of 21 categories are related to Indian traditional clothes and represents the most available stock. The data available allows to look into the business in India and check on the behavior of those categories.
'''
#Loading the data
amazon_sales_zip = zipfile.ZipFile('./A1/Amazon Sale Report.csv.zip')
amazon_sales = pd.read_csv(amazon_sales_zip.open('Amazon Sale Report.csv'), encoding = 'iso-8859-1')


#Pouring table with columns needed.
amazon_s_world = amazon_sales[['Status',
                               'Category', 
                               'Qty',
                               'Amount']]

#Identifying effective sells as those that weren't cancelled using a new column with labels
amazon_s_world['effective_sells'] = amazon_s_world.apply(lambda x: 
                                                         # value 0 if the order was cancelled
                                                         0 if 'Cancelled' in x['Status'] 
                                                         # value 1 if the order was sucessful
                                                         else 1, axis=1)

#Filtering the sucessful orders: Amount was sold if effective_sells is 1
amazon_s_world['amount_sold'] = amazon_s_world.apply(lambda x:
                                                     #Keep te same value as original if there was an effective sell
                                                     x['Amount'] if x['effective_sells']>0
                                                     # 0 units sold if it wasn't effective
                                                     else 0, axis=1)

#Filtering a final table with 'valid transactions'
amazon_cleaned=amazon_s_world[amazon_s_world['amount_sold']>0]

#Calculating units sold by Category
india_sells=amazon_cleaned[['Category',
                            'amount_sold']]

#Calculating number of items sold. Left as a number to calculate percentages later
cat_sells=india_sells.groupby('Category')['amount_sold'].sum()

#Converting into dataframe
df_cat_sells = pd.DataFrame(cat_sells).reset_index()

#Total units sold
transactions_total=india_sells['amount_sold'].sum()

#Column with Categories's percentages over all the sells
df_cat_sells['Percentage']= round(df_cat_sells['amount_sold']/transactions_total*100,2)

#Checking if there is 100% percentage
df_cat_sells['Percentage'].sum()

#Sorting by most popular categories
df_cat_sells.sort_values(by=['Percentage'],
                         ascending=False)
'''
In India's market, the most popular category is the traditional set followed by kurtas, with a similar demand pattern as the stock availability for the overall markets. Categories that may be related to sport clothing aren't representative from we we can assume that the company is addressing the market with a different strategy (assumptions for more information).

As India seems to represent an important market for the company to adjust drastically its portfolio, the general retail clothing market is analyzed as well.
'''
#Loading data
data_supply_zip = zipfile.ZipFile('./A1/DataCoSupplyChainDataset.csv.zip')
data_supply = pd.read_csv(data_supply_zip.open('DataCoSupplyChainDataset.csv'), encoding = 'iso-8859-1')


supply_data=data_supply
#Cleanning Date format dt package
#Defining the column 'date' as a datetime format
supply_data['date']=pd.to_datetime(supply_data['shipping date (DateOrders)'])
#Extracting the month from date
supply_data['month'] = supply_data['date'].dt.month
#Extracting the year from date
supply_data['year'] = supply_data['date'].dt.year

#Selecting columns to use
market=supply_data[['month','year',
                    'Type','Late_delivery_risk',
                    'Category Name',
                    'Customer Country',
                    'Market',
                    'Order Country',
                    'Order Profit Per Order']]

#Filtering the data
#Bringing the transactions placed in India
filtered_market1 = market[market['Order Country'].str.contains('India')]
#Bringing the sells in Clothing categories
filtered_market2=filtered_market1[filtered_market1['Category Name']
                                  .str.contains('Clothing')]
#Grouping the average profit per order by category
filtered_market3=filtered_market2.groupby(['Category Name',
                                           'year'])['Order Profit Per Order'].mean()

#Organizing results in a dataframe
india_market=pd.DataFrame(filtered_market3).reset_index()

#Limiting the decimanls
india_market['Order Profit Per Order']=round(india_market
                                             ['Order Profit Per Order'],
                                             2)

#Sorting the dataframe
india_market.sort_values(by=['year',
                             'Order Profit Per Order'], 
                         ascending=[True,True])
'''
The clothing market in India seems to have low profits. Therefore, the sells The Company is having in this country may be generating loss revenue despite its huge amounts, or managing a good economy of scale to support the overall production.
'''
## References
Britannica. (n.d.). Trinidad and Tobago. In Encyclop√¶dia Britannica. Retrieved February 22, 2023, from https://www.britannica.com/place/Trinidad-and-Tobago/additional-info#contributors

UNDP. (2021). Human Development Index. Retrieved October 20, 2021, from https://hdr.undp.org/data-center/human-development-index#/indicies/HDI
